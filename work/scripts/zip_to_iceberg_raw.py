import os, zipfile, tempfile, traceback, sys, shutil
from pathlib import Path
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp

ZIP_DIR      = Path(os.environ.get("ZIP_DIR", "/win_df"))
CSV_SEP      = os.environ.get("CSV_SEP", ",")
CSV_HEADER   = os.environ.get("CSV_HEADER", "true")
INFER_SCHEMA = os.environ.get("INFER_SCHEMA", "true")
CSV_ENCODING = os.environ.get("CSV_ENCODING", "UTF-8")
TABLE_NAME   = os.environ.get("TABLE_NAME", "ice.bronze.crpt_2025_raw")

# --- STREAM COPY: –∫–æ–ø–∏—Ä—É–µ–º –ø–æ –∫—É—Å–∫–∞–º, –±–µ–∑ —á—Ç–µ–Ω–∏—è (.read()) –≤—Å–µ–≥–æ —Ñ–∞–π–ª–∞ ---
def first_csv_from_zip(zp: Path, dst_dir: Path) -> Path | None:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π CSV-—Ñ–∞–π–ª –∏–∑ –∞—Ä—Ö–∏–≤–∞ ZIP –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–≥–æ –≤ —É–∫–∞–∑–∞–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        zp (Path): –ü—É—Ç—å –∫ ZIP-–∞—Ä—Ö–∏–≤—É, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –±—É–¥–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω CSV-—Ñ–∞–π–ª.
        dst_dir (Path): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –≤ –∫–æ—Ç–æ—Ä—É—é –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π CSV-—Ñ–∞–π–ª.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        Path | None: –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É CSV-—Ñ–∞–π–ª—É –≤ —Å–ª—É—á–∞–µ —É—Å–ø–µ—Ö–∞, 
                     –∏–ª–∏ None, –µ—Å–ª–∏ CSV-—Ñ–∞–π–ª—ã –≤ –∞—Ä—Ö–∏–≤–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.

    –ò—Å–∫–ª—é—á–µ–Ω–∏—è:
        - –ú–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–∏ ZIP-–∞—Ä—Ö–∏–≤–∞.
        - –ú–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ø—Ä–∞–≤ –Ω–∞ —á—Ç–µ–Ω–∏–µ/–∑–∞–ø–∏—Å—å —Ñ–∞–π–ª–æ–≤.

    –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±—É—Ñ–µ—Ä —Ä–∞–∑–º–µ—Ä–æ–º 16 –ú–ë –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.
        –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ –∏–º—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ ZIP-—Ñ–∞–π–ª–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .csv.
    """
    with zipfile.ZipFile(zp, "r") as z:
        names = [n for n in z.namelist() if n.lower().endswith(".csv")]
        if not names:
            return None
        src = names[0]
        dst_path = dst_dir / f"{zp.stem}.csv"
        with z.open(src, "r") as fin, open(dst_path, "wb") as fout:
            shutil.copyfileobj(fin, fout, length=16 * 1024 * 1024)  # 16MB –±—É—Ñ–µ—Ä
        return dst_path

if __name__ == "__main__":
    # –º–æ–∂–Ω–æ —á—É—Ç—å –ø–æ–º–æ—á—å Iceberg –Ω–∞ —Å–ª—É—á–∞–π —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π —Å—Ö–µ–º
    # Spark 3.5 + Iceberg 1.6 –ø–æ–Ω–∏–º–∞—é—Ç —ç—Ç–æ—Ç —Ñ–ª–∞–≥
    # (–æ—Å—Ç–∞–≤—å –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º, –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ)
    # spark.conf.set("spark.sql.iceberg.merge-schema", "true")

    spark = (SparkSession.builder.appName("zip->iceberg(as-is)").getOrCreate())

    zip_paths = sorted(p for p in ZIP_DIR.glob("*.zip"))
    total = len(zip_paths)
    print(f"üëâ [INFO] Found {total} zip files in {ZIP_DIR}", flush=True)
    if total == 0:
        print("üôÖ‚Äç‚ôÇÔ∏è Archives are not found ‚Äî do not create anything, exit")
        spark.stop(); sys.exit(0)
    
    spark.sql("CREATE NAMESPACE IF NOT EXISTS ice.bronze")

    # —Å–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É iceberg –¥–ª—è –ª–æ–≥–æ–≤, –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç
    spark.sql("""
        CREATE TABLE IF NOT EXISTS ice.bronze.crpt_load_log (
          zip_name string,
          loaded_at timestamp
        ) USING iceberg
    """)

    table_exists = spark._jsparkSession.catalog().tableExists(TABLE_NAME)

    # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–∞–ª–∏ –ª–∏ –º—ã —ç—Ç–æ—Ç –∞—Ä—Ö–∏–≤ —Ä–∞–Ω–µ–µ
    processed = skipped = failed = 0
    for i, zp in enumerate(zip_paths, 1):
        try:
            name = zp.name
            esc = name.replace("'", "''")
            already = spark.sql(
                f"SELECT 1 FROM ice.bronze.crpt_load_log WHERE zip_name='{esc}' LIMIT 1"
            ).count() > 0
            if already:
                # –µ—Å–ª–∏ —É–∂–µ –∑–∞–≥—Ä—É–∂–∞–ª–∏, —Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                print(f"[SKIP {i}/{total}] {name}: already logged", flush=True)
                skipped += 1
                continue

            # –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–∞–ª–∏, —Ç–æ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä—É–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, 
            # —Å–æ–∑–¥–∞–µ–º iceberg-—Ç–∞–±–ª–∏—Ü—É –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            print(f"[START {i}/{total}] {name}", flush=True)
            tmpdir = Path(tempfile.mkdtemp(prefix="csv_as_is_"))
            # –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –∞—Ä—Ö–∏–≤–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω CSV
            csv_path = first_csv_from_zip(zp, tmpdir)
            if not csv_path or not csv_path.exists():
                print(f"[WARN  {i}/{total}] {name}: no CSV inside", flush=True)
                skipped += 1
                continue
            # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            size_mb = round(csv_path.stat().st_size / (1024*1024), 2)
            print(f"[EXTRACT {i}/{total}] -> {csv_path.name} ({size_mb} MB)", flush=True)
            # —á–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV
            df = (spark.read
                    .option("sep", CSV_SEP)
                    .option("header", CSV_HEADER)
                    .option("inferSchema", INFER_SCHEMA)
                    .option("encoding", CSV_ENCODING)
                    .csv(str(csv_path)))
            # –µ—Å–ª–∏ –µ—â–µ –Ω–µ —Å–æ–∑–¥–∞–Ω–∞, —Ç–æ —Å–æ–∑–¥–∞–µ–º iceberg-—Ç–∞–±–ª–∏—Ü—É
            if not table_exists:
                df.writeTo(TABLE_NAME).create()
                table_exists = True
                print(f"üëâ [CREATE] {TABLE_NAME}", flush=True)
            # –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            df.writeTo(TABLE_NAME).append()
            print(f"üëâ [APPEND] {name} -> {TABLE_NAME}", flush=True)
            # –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –≤ –ª–æ–≥
            (spark.createDataFrame([(name,)], ["zip_name"])
                 .withColumn("loaded_at", current_timestamp())
                 .writeTo("ice.bronze.crpt_load_log").append())

            processed += 1
            print(f"[DONE  {i}/{total}] {name}", flush=True)

        except Exception as e:
            failed += 1
            print(f"[FAIL  {i}/{total}] {zp.name}: {e}", flush=True)
            traceback.print_exc()
        finally:
            try:
                # —É–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                for p in tmpdir.glob("*"):
                    try: p.unlink()
                    except: pass
                tmpdir.rmdir()
            except Exception:
                pass
    # –ª–æ–≥–∏—Ä—É–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –∑–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –≤ –∫–æ–Ω—Å–æ–ª—å
    logged = spark.sql("SELECT COUNT(*) AS c FROM ice.bronze.crpt_load_log").first()["c"]
    print("===== SUMMARY =====", flush=True)
    print(f"found      : {total}", flush=True)
    print(f"processed  : {processed}", flush=True)
    print(f"skipped    : {skipped}", flush=True)
    print(f"failed     : {failed}", flush=True)
    print(f"logged rows: {logged}", flush=True)
    print("===================", flush=True)
    # –∑–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É
    spark.stop()
    print("‚úÖ [DONE]", flush=True)