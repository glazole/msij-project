services:
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://minio:9000/minio/health/ready"]
    networks: [spark_network]
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio_minio
      AWS_REGION: us-east-1
      AWS_DEFAULT_REGION: us-east-1
    ports: ["9000:9000", "9001:9001"]
    volumes:
      - /home/glazole/msij-project/minio:/data

  mc:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    networks: [spark_network]
    entrypoint: >
      /bin/sh -c "
      sleep 3 &&
      mc alias set local http://minio:9000 minio minio_minio &&
      mc mb -p local/raw local/stage local/warehouse || true &&
      tail -f /dev/null
      "

  spark-master:
    build: ./docker/spark
    image: local/spark-rest:3.5.0-aws34-ice161-rest160
    container_name: spark-master
    hostname: spark-master
    networks: [spark_network]
    depends_on:
      minio:
        condition: service_healthy
      mc:
        condition: service_started
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: 7077
      SPARK_UI_PORT: 8080
      AWS_REGION: us-east-1
      AWS_DEFAULT_REGION: us-east-1
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio_minio
      AWS_EC2_METADATA_DISABLED: "true"
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - /home/glazole/msij-project/conf/spark:/opt/bitnami/spark/conf
      - /home/glazole/msij-project/work:/work
      - /mnt/e/crpt_2025:/win_df

  spark-worker:
    image: local/spark-rest:3.5.0-aws34-ice161-rest160
    container_name: spark-worker
    hostname: spark-worker
    networks: [spark_network]
    depends_on:
      spark-master:
        condition: service_started
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 4G
      SPARK_WORKER_CORES: 2
      AWS_REGION: us-east-1
      AWS_DEFAULT_REGION: us-east-1
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio_minio
      AWS_EC2_METADATA_DISABLED: "true"
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 5G
    ports:
      - "8081:8081"
    volumes:
      - /home/glazole/msij-project/conf/spark:/opt/bitnami/spark/conf
      - /home/glazole/msij-project/work:/work
      - /mnt/e/crpt_2025:/win_df

  jupyter:
    image: local/jupyter-spark-rest:3.5.0-aws34-ice161-rest160
    build:
      context: .
      dockerfile: docker/jupyter/Dockerfile
    container_name: jupyter
    hostname: jupyter
    networks: [spark_network]
    depends_on:
      spark-master:
        condition: service_started
    ports:
      - "8888:8888"
      - "4040:4040"
      - "4041:4041"
    working_dir: /work
    environment:
      JUPYTER_TOKEN: "lab"
      AWS_REGION: us-east-1
      AWS_DEFAULT_REGION: us-east-1
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio_minio
      AWS_EC2_METADATA_DISABLED: "true"
      HOME: /work
      JUPYTER_CONFIG_DIR: /work/.jupyter
      JUPYTER_DATA_DIR: /work/.jupyter
      JUPYTER_RUNTIME_DIR: /work/.jupyter/runtime
      JUPYTERLAB_WORKSPACES_DIR: /work/.jupyter/lab/workspaces
      SPARK_SUBMIT_OPTS: "-Dfs.s3a.metrics.skip=true"
      PYSPARK_SUBMIT_ARGS: >
        --master spark://spark-master:7077
        --conf spark.driver.bindAddress=0.0.0.0
        --conf spark.driver.host=jupyter
        pyspark-shell
    command: >
      bash -c "
        mkdir -p /work/.jupyter/lab/workspaces /work/.jupyter/lab/user-settings /work/.jupyter/runtime &&
        chown -R 1001:1001 /work/.jupyter &&
        jupyter lab --ip=0.0.0.0 --no-browser
        --NotebookApp.notebook_dir=/work
        --ServerApp.token=$$JUPYTER_TOKEN
      "
    volumes:
      - /home/glazole/msij-project/conf/spark:/opt/bitnami/spark/conf
      - /home/glazole/msij-project/work:/work

  # === PostgreSQL для Iceberg Catalog ===
  postgresql:
    image: postgres:15-alpine
    container_name: postgresql
    hostname: postgresql
    networks: [spark_network]
    environment:
      POSTGRES_DB: iceberg_catalog
      POSTGRES_USER: iceberg
      POSTGRES_PASSWORD: iceberg
    ports:
      - "5432:5432"
    volumes:
      - pg_iceberg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U iceberg -d iceberg_catalog"]
      interval: 10s
      timeout: 5s
      retries: 5

  # === Iceberg REST Catalog ===
  iceberg-rest:
    image: tabulario/iceberg-rest:1.6.0
    container_name: iceberg-rest
    hostname: iceberg-rest
    networks: [spark_network]
    depends_on:
      postgresql:
        condition: service_healthy
      spark-master:
        condition: service_started
    ports:
      - "8181:8181"
    environment:
      # === Основные параметры каталога ===
      CATALOG_WAREHOUSE: s3://warehouse/iceberg/
      CATALOG_IO_IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_CATALOG_IMPL: org.apache.iceberg.jdbc.JdbcCatalog
      CATALOG_JDBC_DRIVER: org.postgresql.Driver
      CATALOG_JDBC_USER: iceberg
      CATALOG_JDBC_PASSWORD: iceberg
      CATALOG_JDBC_SCHEMA_VERSION: V1
      CATALOG_URI: jdbc:postgresql://postgresql:5432/iceberg_catalog
      # === Настройки доступа к MinIO ===
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH_STYLE_ACCESS: "true"
      CATALOG_S3_ACCESS_KEY: minio
      CATALOG_S3_SECRET_KEY: minio_minio
      CATALOG_S3_CONNECTION_SSL_ENABLED: "false"
      CATALOG_S3_REGION: us-east-1
      # === Дублируем без префикса (для S3FileIO в некоторых контекстах) ===
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: minio
      S3_SECRET_KEY: minio_minio
      S3_PATH_STYLE_ACCESS: "true"
      S3_CONNECTION_SSL_ENABLED: "false"
      S3_REGION: us-east-1
      # --- AWS совместимость (опционально, не мешает)
      AWS_ENDPOINT_URL: http://minio:9000
      AWS_S3_ENDPOINT: http://minio:9000
      AWS_REGION: us-east-1
      AWS_DEFAULT_REGION: us-east-1
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio_minio

      LOG_LEVEL: INFO

    volumes:
      - /home/glazole/msij-project/conf/spark:/opt/spark/conf
      - /home/glazole/msij-project/work:/work

volumes:
  minio-data:
  spark-logs:
  pg_iceberg_data:

networks:
  spark_network:
    driver: bridge
